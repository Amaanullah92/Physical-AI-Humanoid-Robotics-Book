---
id: 001
title: Specify RAG Chatbot for Book
stage: spec
date: 2025-12-17
surface: {{SURFACE}}
model: {{MODEL}}
feature: 002-rag-chatbot-book
branch: 002-rag-chatbot-book
user: {{USER}}
command: {{COMMAND}}
labels: [{{LABELS}}]
links:
  spec: {{LINKS_SPEC}}
  ticket: {{LINKS_TICKET}}
  adr: {{LINKS_ADR}}
  pr: {{LINKS_PR}}
files:
{{FILES_YAML}}
tests:
{{TESTS_YAML}}
---

## Prompt

# Project RAG Chatbot for "Physical AI & Humanoid Robotics" Docusaurus Book # Short description Automatically ingest the Docusaurus book at https://amaanullah92.github.io/Physical-AI-Humanoid-Robotics-Book/sitemap.xml, embed content into Qdrant using Cohere embeddings, and expose a FastAPI-backed RAG agent (OpenAI Agents / ChatKit SDK compatible with Gemini) that powers an embeddable chatbot in the published book. The agent must support both global retrieval and strictly **user-selected-text-only** answering. # Audience Developers and maintainers of the book site; end-users (readers, students) who ask factual questions about the book. # Primary objectives 1. Idempotent ingestion pipeline: sitemap â†’ fetch pages â†’ clean â†’ chunk â†’ embed â†’ store. 2. Embedding pipeline: batch to Cohere, validate vectors, store in Qdrant collection. 3. Retrieval and QA: agent uses retrieved chunks to generate grounded answers. 4. Selection-based grounding: when user selects text, answer must be based **only** on that selection. 5. Web integration: provide a chat widget component for Docusaurus to call the FastAPI `/query` endpoint. # External services (configuration via env) - COHERE_API_KEY (secret) â€” for embeddings - COHERE_EMBED_MODEL (string) â€” e.g. `embed-english-v2` or provider model name; configurable - QDRANT_ENDPOINT: https://e16d5a52-6b5b-4c6d-ba5d-7798e6c53673.us-east4-0.gcp.cloud.qdrant.io - QDRANT_API_KEY (secret) - GEMINI_API_KEY (secret) â€” used via OpenAI-compatible client (OpenAI Agents SDK) - AGENT_BASE_URL (optional) â€” if using alternate Gemini base URL - FASTAPI_PORT (default 8000) - ENV (development | production) > **Security note:** Do NOT place keys in repo. `.env` only, secrets manager for production. # Non-goals - Fine-tuning LLMs - Indexing external non-book sources (unless explicitly added by owner) - Serving private user data # Success criteria / Acceptance tests 1. Ingest pipeline runs and creates Qdrant collection `physical_ai_book` with: at least N vectors (N = number of meaningful chunks from sitemap). 2. Query `/query` with a question about a book topic returns an answer citing source `url` and `section` for each grounded claim. 3. Selecting a paragraph on the site and toggling "selected-only" yields answers strictly about that selection; agent refuses if selection insufficient. 4. Re-ingestion is idempotent: running `/ingest` twice does not produce duplicate vectors (use deterministic id per chunk). 5. Latency: median query time < 1.5s for retrieval + LLM call in dev environment (note: subject to LLM provider). 6. Logging exists for ingestion, embedding, qdrant writes, retrieval hits, and agent responses. # Constraints - Chunk size: target 500â€“800 tokens (approx. 300â€“600 words depending on content). Overlap: ~100 tokens. - Vector dim: must match chosen Cohere model â€” validate prior to storing. - Collection name: `physical_ai_book` - Top-k default retrieval: 5 (configurable) - Use API keys only from env. No secrets in spec file. # High-level pipeline & endpoints ## Pipeline (ingest) - Endpoint: `POST /ingest` - Steps: 1. Fetch sitemap.xml (configurable URL) 2. Extract page URLs; filter out non-content (assets, tag pages) 3. For each page: - Fetch HTML - Extract readable text + metadata (title, headings) - Normalize whitespace; remove navigation boilerplate - Split into sections based on headings - Chunk each section (size/overlap) - Compute deterministic chunk id: `sha256(url + section_path + chunk_index)` 4. Batch embedding via Cohere 5. Upsert vectors to Qdrant with payload: - `text` (chunk text) - `url` - `title` - `section_heading` - `chunk_index` - `chunk_id` 6. Return summary report: pages processed, chunks created, vectors upserted. ## API (FastAPI) - `POST /ingest` â€” trigger ingestion (idempotent) - `POST /query` - body: ```json { "query": "How do servo motors in the book differ from stepper motors?", "top_k": 5, "selected_text": null, // string; if present, use selection-only flow "filter_url": null // optional URL to restrict retrieval } ``` - response: ```json { "answer": "string", "sources": [ { "url": "string", "title": "string", "section": "string", "score": 0.92 } ], "tool_calls": [ ... ], "raw_agent_output": { ... } } ``` # Retrieval & agent behavior (detailed) - Retrieval tool: - If `selected_text` provided: - Bypass Qdrant similarity search. - Chunk `selected_text` locally (same chunk rules) and embed. - Provide those vectors as retrieval set (or pass text directly to agent as context) â€” agent must only use this context. - Else: - Convert `query` to embedding - Query Qdrant with top_k, optionally filter by `filter_url` - Agent: - Implement using OpenAI Agents SDK (Runner / Agent pattern from your reference code) - Tools registered: - `retrieve_from_qdrant(query, top_k, filter_url)` â€” returns chunks with payloads - `summarize_chunks(chunks)` â€” optional tool for long context - Prompt template MUST enforce: - "Only use the provided chunks. Cite exact `url` + `section_heading` when asserting facts. If chunks do not provide sufficient basis, reply: 'Insufficient information in the provided context to answer that question.' Do not hallucinate." - Agent must include sources block in final output (url + snippet + score). # Qdrant details - Collection: `physical_ai_book` - Distance: cosine - Payload schema: - text: string - url: keyword - title: string - section_heading: string - chunk_index: integer - chunk_id: string - Upsert should be batched (e.g., 256 vectors per request) - Implement an index/cleanup job that deletes vectors older than X days only if `--purge` provided # Chunking rules (concrete) - Use sentence-aware chunking (prefer not to cut sentences). - Max tokens per chunk: 800 tokens; overlap: 100 tokens. - For code blocks, preserve formatting and treat code-blocks as separate chunks if > 200 tokens. # Monitoring & logging - Ingest logs: page url, status, num_chunks, time_taken - Embedding logs: batch_id, model_name, batch_size, failures - Qdrant logs: upsert_count, result - Query logs: query_text (hashed for PII), top_k, selected_text_flag, returned_sources_count, latency - Error alerts: failed embedding batch, qdrant upsert > threshold, ingestion timeouts # Testing (automated) - Unit tests: - html -> cleaned text extractor yields expected section counts for sample pages - chunker preserves sentence boundaries and overlap - embedding pipeline calls Cohere (mocked) and returns correct dims - qdrant upsert called with deterministic IDs - Integration tests: - Ingestion end-to-end for a small subset of sitemap (mocked pages) - Query endpoint returns answer with at least one source for known queries - Acceptance tests: - Selected-text flow returns only selection-based answers for a set of queries # Deployment & infra - Local dev: Docker compose with: - FastAPI app - Local Qdrant (optional) or point at cloud Qdrant - Production: - FastAPI behind a small serverless or container service (Vercel serverless functions, Cloud Run, or similar) - Secrets in provider's secrets manager - Use Qdrant Cloud endpoint (provided) - Rate limiting: prevent mass re-ingest calls; require auth token for `/ingest` in prod # UX / Frontend integration - Provide a lightweight React widget to embed in Docusaurus: - Chat input, conversation pane - Toggle: â€œAnswer only from selected textâ€ (if toggled, widget should send `selected_text` to `/query`) - Display sources with clickable URLs and the snippet - Highlighted source snippet when hovering source in chat - Accessibility: keyboard accessible, screen-reader labels # Deliverables 1. `/ingest` pipeline code (Python) 2. FastAPI server with `/query` and `/ingest` 3. Agent code (OpenAI Agents SDK compatible) 4. Qdrant schema & upsert scripts 5. React chat widget for Docusaurus 6. README: env setup, run, deploy steps, acceptance test runbook 7. Unit & integration tests # Milestones (spec-driven) - M1 â€” Spec validation & architecture diagram (this spec) - M2 â€” Ingest: sitemap â†’ chunk â†’ embed â†’ qdrant upsert (unit tests) - M3 â€” Retrieval tool + Qdrant query (integration test) - M4 â€” Agent wiring + prompt templates + test harness - M5 â€” FastAPI endpoints + CI tests - M6 â€” Docusaurus chat widget + embed & end-to-end test - M7 â€” Hardening, logging, docs, deployment scripts # Implementation notes & examples - Deterministic chunk id example: - `chunk_id = sha256(url + '||' + section_heading + '||' + str(chunk_index))` - Retrieval sample flow: 1. Query â†’ embed â†’ qdrant search top_k â†’ return payloads â†’ agent receives chunks. - Selected-text flow sample: 1. Widget sends `selected_text` â†’ backend chunks & embeds selection (or passes raw selection) â†’ agent given ONLY selection as context. # Open questions (for implementer, not blocking) - Should we persist embeddings for user-selected text? (Default: no â€” ephemeral) - Allowed max size of `selected_text` in bytes for the widget (default: 32KB) - Authentication model for `/ingest` in production # Operational checks (pre-deploy) - Validate COHERE_EMBED_MODEL dims vs Qdrant vector_size - Test ingestion on a small subset of sitemap - Confirm CORS for Docusaurus domain # Notes - Do not commit secrets. - Keep ingestion idempotent and resumable.

## Response snapshot

Feature specification created for RAG chatbot.

## Outcome

- âœ… Impact: {{OUTCOME_IMPACT}}
- ðŸ§ª Tests: {{TESTS_SUMMARY}}
- ðŸ“ Files: {{FILES_SUMMARY}}
- ðŸ” Next prompts: {{NEXT_PROMPTS}}
- ðŸ§  Reflection: {{REFLECTION_NOTE}}

## Evaluation notes (flywheel)

- Failure modes observed: {{FAILURE_MODES}}
- Graders run and results (PASS/FAIL): {{GRADER_RESULTS}}
- Prompt variant (if applicable): {{PROMPT_VARIANT_ID}}
- Next experiment (smallest change to try): {{NEXT_EXPERIMENT}}
